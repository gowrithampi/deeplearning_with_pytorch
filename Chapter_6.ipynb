{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chapter 6.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOwVjDagj8JpyLw34PTmElL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gowrithampi/deeplearning_with_pytorch/blob/main/Chapter_6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LxxZcdjh2ZYa"
      },
      "source": [
        "#Using a Neural Network to Fit the Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KlXxuJ-p2lfz"
      },
      "source": [
        "The following is my attempt to provide some intutition around one of the reasons deep learning is useful, namely the lack of a need feature engineering. The intuition was gained by reading this book and several other works. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bE_k40xegezM"
      },
      "source": [
        "## Artificial Neurons\n",
        "loosely inspired by the working of the human brain, aritifical neurons are at its core a linear transofrmation followed by a non linear *activation function*. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mYMWA41_ho7a"
      },
      "source": [
        "The Neuron: $o = tanh(w*x+b)$\n",
        "\n",
        "$x$ *is the input*\n",
        "\n",
        "$w$ *and* $b$ *are the learned parameters*\n",
        "\n",
        "$w*x+b$ is the linear transformation \n",
        "\n",
        "$tanh$ is the non linear activation function \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_0WqlK5Z2OQ6"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fXbZP4TBi11h"
      },
      "source": [
        "The non linear activation function serves two purposes, which are important for approximating complex functions: \n",
        "\n",
        "1. The non linearity means the slope of the output with respect to the parameter( a weight) can change depending on the value of the output. \n",
        "\n",
        "2. The bounds provided by many of the activation functions, ensures that the output remains within a range. For eg: the output of a tanh funciton remains between -1 and +1\n",
        "\n",
        "The output is sensitive to the weights only in a region "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "69mzvrOm1gL-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}