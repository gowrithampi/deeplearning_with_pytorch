{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chapter 5: The mechanics of learning.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNz01cpP11dZ0Jy0SrFHEm7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gowrithampi/deeplearning_with_pytorch/blob/main/Chapter_5_The_mechanics_of_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-xN4Uzoyt9P3"
      },
      "source": [
        "# Deep Learning with Pytorch\n",
        "## Chapter 5: The Mechanics of Learning\n",
        "\n",
        "### This is based entirely on the Text Book: \n",
        "### Deep Learning with PyTorch by \n",
        "* Eli Stevens\n",
        "* Luca Antiga\n",
        "* Thomas Viehmann\n",
        "\n",
        "I added my own mini experiments, insights, an eye into my struggles and my understanding of the text as I move through it. However phrases, sentences from the author may creep in. I don't claim this to be adding significantly to their work, neither do I seek to gain monetary compensation from this effort.  \n",
        "The original code in the textbook is available here:\n",
        "\n",
        "https://github.com/deep-learning-with-pytorch/dlwpt-code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3igl2CsVid8R"
      },
      "source": [
        "This is a fun chapter it familiarizes one with the idea of learning, by building a simple linear regression model by gradient descent. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xcLxvu9ztote"
      },
      "source": [
        "Problem statement : There is a cool thermometer hanging on the wall. It seems to measure temperature, but we don't know what unit it measures temperature in. We want a way to convert the temperature shown by the thermometer to celsius. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Owxqm_WYla1e"
      },
      "source": [
        "Our strategy: We observe a set of temperature data, from our little pocket celsius thermometer and the corresponding reading from the wall thermometer. Armed with this data, we tackle this problem as a machine learning problem.\n",
        "Assuming there is a linear relationship between the readings from both thermometers, we use gradient descent to learn the weights of our model. \n",
        "First without pytorch's autograd and then with it. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yX2PSwBgvFZ8"
      },
      "source": [
        "# t_c is a list of temperatures observed in celsius\n",
        "# t_u is a list of temperatures observed in unknown units \n",
        "import torch\n",
        "t_c = [0.5, 14.0, 15.0,28.0,11.0,8.0,3.0,-4.0,6.0,13.0,21.0]\n",
        "t_u = [35.7,55.9,58.2,81.9,56.3,48.9,33.9,21.8,48.4,60.4,68.4]\n",
        "t_c = torch.tensor(t_c)\n",
        "t_u = torch.tensor(t_u)\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wel7AtmjwZ1W"
      },
      "source": [
        "Let's visualize above data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "pznLnRc5wdpB",
        "outputId": "2c5c1044-59ad-4b0b-bfad-40c0930fe650"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "\n",
        "plt.scatter(t_u, t_c)\n",
        "plt.title('Temperature scatter plot')\n",
        "ax.set_xlabel('Temperature in unknown units')\n",
        "ax.set_ylabel('Temperature in celsius')\n",
        "plt.show()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAEWCAYAAABv+EDhAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3de5gcZZn+8e9NDjIQYIDESIZDEDEajSY6ICgqCBpRhMjFgri64CkquopihLj+NLK6UYMgij8xCIKIysEQEdEIyFmFnRCWBDAqGg6TAIMQEnCEJDz7R71NKrMzk+rJdPf01P25rr666/y80z39dL1V9ZQiAjMzK5+tGh2AmZk1hhOAmVlJOQGYmZWUE4CZWUk5AZiZlZQTgJlZSTkBmNmASLpe0gcbHYcNnBOA9UvSk7nHs5K6c8P/2uj4BkLSCkmHNDqOgZI0UVJIGpkbd7ykmxsZV196i9eGBr8h1q+IGFN5LWkF8MGIuKZxEfVP0siIWN/s26i34dgm2zzvAdiASNpK0imS7pX0d0mXSNopTav84nufpAckPS7pI5L2kXSnpNWSzsqt63hJt0g6S9ITkv4o6eDc9B0knStplaROSV+WNKLHsmdI+jswR9Jekn6b4npU0kWSWtP8FwK7A79IezGflXSgpAd7tO+5vQRJcyRdJulHktYAx/cXUy9/q30ldUhaI+lhSafnph0g6Xfpb/KApOPT+LdLWpKWeUDSnNwqb0zPq1Mb9gfOBvZPw6vTOp4n6TRJ96ftni2pJU07UNKDkk6W9BDwg17i7vd96eXz8HlJ90l6RNIPJe3QT7w2BDgB2ED9OzADeCMwAXgc+E6PeV4D7A0cA3wT+A/gEOBlwNGS3thj3nuBscAXgQWVhAKcD6wHXgRMA94CfLDHsn8FxgNfAQTMTXG9FNgNmAMQEe8F7gfeERFjIuLrBdt7BHAZ0ApcVCCmvDOBMyNie2Av4BIASXsAvwK+DYwDpgJ3pGWeAv4tbe/twEclzUjT3pCeW1Mbfg98BPh9Gm5N078KvDit90VAG/CFXFwvAHYC9gBm9hF7f+9L3vHpcRDwQmAMUEnyvcVrQ0FE+OFHoQewAjgkvb4HODg3bRdgHVm34kQggLbc9L8Dx+SGfwacmF4fD6wElJt+G/Besi/1p4GW3LRjgetyy96/mbhnAEt6a0caPhB4sJ+2zgFuzE3rN6Zetn8j8CVgbI/xs4HLC/7tvwmckV5X/r4jc9OPB27ODYssieyVG7c/8Ldcm58Btu5nm32+L+n19WRdggDXAifk5pvUy+dhZJG2+lG/h48B2EDtAVwu6dncuA1kX44VD+ded/cyPCY33BnpmyO5j+wX/B7AKGCVpMq0rYAHcvPmXyNpPNmv7tcD26X5Hy/Uqr7lt1EkprwPAKcCf5T0N+BLEXEl2Z7Jvb0tIOk1ZL/gXw6MBp4HXFpFvOOAbYDFuRgF5LupuiLin5tZT1/vS08T0rT8fCPZ9PNgQ4y7gGygHgAOjYjW3GPriOgc4PralPumIuunX5m28zTZr+fKdraPiJfl5u1Z0va/0rgpkXW7vIfsy6+v+Z8i+7IEIPXlj+sxT36ZIjFtXDDizxFxLPB84GvAZZK2TevZq7dlgB8DVwC7RcQOZH38lTb0VsK357hHyZLsy3Ix7hC5g/p9rKenvt6XnlaSJcb8fOvJkr5LDg9RTgA2UGcDX0n92EgaJ+mILVjf84FPSBol6V/I+u6viohVwG+Ab0jaPh1s3KvH8YOetgOeBJ6Q1AbM6jH9YbJ+6oo/AVunA6+jgM+T/eLuVbUxSXqPpHER8SywOo1+luxYwiGSjpY0UtLOkqbm2vBYRPxT0r7Au3Or7ErL59vwMLCrpNEpxmeBc4AzJD0/xdEmaXpf7epDr+9LL/P9BPiUpD0ljSFLwhdHdmZRb/HaEOAEYAN1Jtkv1N9IWgv8geyA4UDdSnbA+FGyA7lHRcTf07R/I+sGuZusK+cysmMOffkS8CrgCeCXwIIe0+cCn09n3nwmIp4ATgC+D3SS7RE8SP+qiemtwF2SniT7u70rIroj4n7gbcBJwGNkB4BfmZY5ATg1/W2/QDpwDBAR/yD7G92S2rAf8FvgLuAhSY+mWU8G/gL8IZ29dA1Z33w1+ntf8s4DLiQ73vE34J9kJwr0Fa8NAdq0e8+s/tKpjx+MiAMaHYtt5Pdl+PMegJlZSTkBmJmVlLuAzMxKynsAZmYl1VQXgo0dOzYmTpzY6DDMzJrK4sWLH42Inte2NFcCmDhxIh0dHY0Ow8ysqUi6r7fx7gIyMyspJwAzs5JyAjAzKyknADOzknICMDMrqaY6C8jMrGwWLulk3qLlrFzdzYTWFmZNn8SMaW2Dsm4nADOzIWrhkk5mL1hK97oNAHSu7mb2gqUAg5IE3AVkZjZEzVu0/Lkv/4rudRuYt2j5oKzfCcDMbIhaubq7qvHVcgIwMxuiJrS2VDW+Wk4AZmZD1Kzpk2gZNWKTcS2jRjBrerU3duudDwKbmQ1RlQO9PgvIzKyEZkxrG7Qv/J7cBWRmVlJOAGZmJeUEYGZWUk4AZmYl5QRgZlZSTgBmZiXlBGBmVlJOAGZmJeUEYGZWUk4AZmYl5QRgZlZSNU8AknaTdJ2kuyXdJemTafwcSZ2S7kiPt9U6FjMz26gexeDWAydFxO2StgMWS7o6TTsjIk6rQwxmZtZDzRNARKwCVqXXayXdA9SmtJ2ZmRVW12MAkiYC04Bb06iPS7pT0nmSduxjmZmSOiR1dHV11SlSM7Phr24JQNIY4GfAiRGxBvgusBcwlWwP4Ru9LRcR8yOiPSLax40bV69wzcyGvbokAEmjyL78L4qIBQAR8XBEbIiIZ4FzgH3rEYuZmWXqcRaQgHOBeyLi9Nz4XXKzvRNYVutYzMxso3qcBfQ64L3AUkl3pHGfA46VNBUIYAXw4TrEYmZmST3OAroZUC+Trqr1ts3MrG++EtjMrKScAMzMSsoJwMyspJwAzMxKygnAzKyknADMzErKCcDMrKScAMzMSsoJwMyspJwAzMxKygnAzKyknADMzErKCcDMrKScAMzMSsoJwMyspOpxQxgzs0IWLulk3qLlrFzdzYTWFmZNn8SMaW2NDmvYcgIwsyFh4ZJOZi9YSve6DQB0ru5m9oKlAE4CNeIuIDMbEuYtWv7cl39F97oNzFu0vEERDX9OAGY2JKxc3V3VeNtyVSUASVtJ2r5WwZhZeU1obalqvG25zSYAST+WtL2kbYFlwN2SZtU+NDMrk1nTJ9EyasQm41pGjWDW9EkNimj4K7IHMDki1gAzgF8BewLvrWlUZlY6M6a1MffIKbS1tiCgrbWFuUdO8QHgGipyFtAoSaPIEsBZEbFOUhTdgKTdgB8C44EA5kfEmZJ2Ai4GJgIrgKMj4vEq4zezYWTGtDZ/4ddRkT2A75F9QW8L3ChpD2BNFdtYD5wUEZOB/YCPSZoMnAJcGxF7A9emYTMzq5PNJoCI+FZEtEXE2yJzH3BQ0Q1ExKqIuD29XgvcA7QBRwAXpNkuINvDMDOzOtlsF5CkL/Qx6dRqNyZpIjANuBUYHxGr0qSHyLqIeltmJjATYPfdd692k2Zm1ociXUBP5R4bgEPJ+u2rImkM8DPgxHRQ+TkREWTHB/6PiJgfEe0R0T5u3LhqN2tmZn3Y7B5ARHwjPyzpNGBRNRtJB5F/BlwUEQvS6Icl7RIRqyTtAjxSzTrNzGzLDORK4G2AXYvOLEnAucA9EXF6btIVwHHp9XHAzwcQi5mZDVCRYwBL2dg9MwIYR3X9/68ju25gqaQ70rjPAV8FLpH0AeA+4Ogq1mlmZluoyHUAh+Verwcejoj1RTcQETcD6mPywUXXY2Zmg6vPBCBp+3Swdm2PSdtLIiIeq21oZmZWS/3tAfyY7Nf/YrIuoPyv+ABeWMO4zMysxvpMABFxWHres37hmJlZvRSpBvq6VAkUSe+RdLokX5FlZtbkipwG+l3gH5JeCZwE3AtcWNOozMys5ookgPXpSt0jyKqBfgfYrrZhmZlZrRU5DXStpNnAe4A3SNoKGFXbsMzMrNaK7AEcAzwNfCAiHiK7CnheTaMyM7OaK1IL6CHg9Nzw/WQ3eDEzsybW34Vga+m9QqfICnj65vBmZk2sv+sAfKDXzGwYK1QNVNIBkt6XXo+V5IvDzMyaXJELwb4InAzMTqNGAz+qZVBmZlZ7RfYA3gkcTnZHMCJiJb4OwMys6RVJAM/kb9lYKQthZmbNrUgCuETS94BWSR8CrgHOqW1YZmZWa0WuAzhN0puBNcAk4AsRcXXNIzMzs5oqckvIPYGbKl/6klokTYyIFbUOzszMaqdIF9ClwLO54Q1pnJmZNbEiCWBkRDxTGUivR9cuJDMzq4ciCaBL0uGVAUlHAI/WLiQzM6uHIgngI8DnJN0v6X6yi8JmFt2ApPMkPSJpWW7cHEmdku5Ij7dVH7qZmW2JImcB3QvsJ2lMGn6yym2cD5zF/60gekZEnFblusysh4VLOpm3aDkrV3czobWFWdMnMWNaW6PDsiZQ5IYwwIC++CvL3Shp4kCWNbP+LVzSyewFS+letwGAztXdzF6wFMBJwDarUDG4Gvm4pDtTF9GODYzDrGnNW7T8uS//iu51G5i3aHmDIrJm0qgE8F1gL2AqsAr4Rl8zSpopqUNSR1dXV73iM2sKK1d3VzXeLK9QF5Ck1wIT8/NHxIDvChYRD+fWfQ5wZT/zzgfmA7S3t/d2gxqz0prQ2kJnL1/2E1pbGhCNNZsi5aAvBE4DDgD2SY/2LdmopF1yg+8ElvU1r5n1bdb0SbSMGrHJuJZRI5g1fVKDIrJmUmQPoB2YnCqCVk3ST4ADgbGSHgS+CBwoaSpZhdEVwIcHsm6z4aias3oq430WkA1EkQSwDHgBWV991SLi2F5GnzuQdZkNdwM5q2fGtDZ/4duAFEkAY4G7Jd0GPF0ZGRGH972ImQ1Ef2f1+EveBluRBDCn1kGYWcZn9Vg9FbkS+IZ6BGJmPqvH6qvPs4Ak3Zye10pak3uslbSmfiGalYfP6rF66nMPICIOSM++AbxZndT6rB7XDbK8wrWAzKw+anVWj+sGWU+NrAVkZnXkukHWkxOAWUn4DCPrqVACkLSHpEPS6xZJPi5g1mT6OpPIZxiVV5FaQB8CLgO+l0btCiysZVBmNvh8hpH1VOQg8MeAfYFbASLiz5KeX9OozGzQuW6Q9VQkATwdEc9IAkDSSLIibmbWZFw3yPKKHAO4QdLngBZJbwYuBX5R27DMzKzWiiSAk4EuYClZ2eargM/XMigzM6u9fruAJI0A7oqIlwDn1CckMzOrh373ACJiA7Bc0u51isfMzOqkyEHgHYG70v0AnqqM9P0AzGrLdXus1ookgP9X8yjMbBOu22P14PsBmA1BvjOY1cNmE4CktWw87380MAp4KiK2r2VgZmXmuj1WD0X2AJ6r+6PsarAjgP1qGZRZ2fnOYFYPVVUDjcxCYHqN4jEzXLfH6qNIF9CRucGtgHbgn0U3IOk84DDgkYh4eRq3E3AxMBFYARwdEY8XjtpsmHPdHqsHRfRf1kfSD3KD68m+sM+JiEcKbUB6A/Ak8MNcAvg68FhEfFXSKcCOEXHy5tbV3t4eHR0dRTZrZmaJpMUR0d5zfJHTQL8fEbf0WNnrgEIJICJulDSxx+gjgAPT6wuA68lKTpiZWZ0UOQbw7YLjqjE+Ilal1w8B4/uaUdJMSR2SOrq6urZws2ZmVtHnHoCk/YHXAuMkfTo3aXtgRO9LVS8iQlKf/VARMR+YD1kX0GBt18ys7PrbAxgNjCFLEtvlHmuAo7Zwuw9L2gUgPRfqTjIzs8HT5x5AugL4BknnR8R9g7zdK4DjgK+m558P8vrNzGwzihwE/oekecDLgK0rIyPiTUU2IOknZAd8x0p6EPgi2Rf/JZI+ANwHHF1l3GZmtoWKJICLyM7ZPwz4CNkv9sJHYyPi2D4mHVx0HWZmNviKnAW0c0ScC6yLiBsi4v1AoV//ZmY2dBXZA1iXnldJejuwEtipdiGZmVk9FEkAX5a0A3AS2fn/2wOfqmlUZmZWc0XuCbx3RFwJPAEcVJeozMys5vpNABGxQdKxwBl1ises7nzrRSurIl1At0g6i+xMoPw9gW+vWVRmdeJbL1qZFUkAU9Pzqblxgc8EsmHAt160MityRzD3+9uw5VsvWplt9joASeMlnSvpV2l4crqC16zp9XWLRd960cqgyIVg5wOLgAlp+E/AibUKyKyefOtFK7MiCWBsRFwCPAsQEeuBDf0vYtYcZkxrY+6RU2hrbUFAW2sLc4+c4v5/K4UiB4GfkrQz2YFfJO1Hdk2A2bAwY1qbv/CtlIokgE+TlW/eS9ItwDi2/H4AZmbWYEXOArpd0huBSYCA5RGxbjOLmZnZELfZBCBpa+AE4ACybqCbJJ0dEf+sdXBmZlY7RbqAfgisZeON4N8NXAj8S62CMjOz2iuSAF4eEZNzw9dJurtWAZnVi2sAWdkVOQ309nTmDwCSXgN01C4ks9qr1ADqXN1NsLEG0MIlnY0OzaxuiiSAVwO/k7RC0grg98A+kpZKurOm0ZnVSH81gMzKokgX0FtrHoVZnbkGkFmBPYCIuA9YA+wA7Fx5RMR9aZpZ03ENILNip4H+J3A8cC/pamBcDtqa3Kzpkza5DwC4BpCVT5EuoKOBvSLimcHeeDqmsJasttD6iGgf7G2Y9aZyto/PArIyK5IAlgGtwCM1iuGgiHi0Rus265NrAFnZFUkAc4ElkpYBT1dGRsThNYvKzMxqrkgCuAD4GrCUVBJ6EAXwG0kBfC8i5vecQdJMYCbA7rvvPsibNzMrryIJ4B8R8a0abf+AiOiU9Hzgakl/jIgb8zOkpDAfoL29PXpbiZmZVa9IArhJ0lyyktD5LqDbt3TjEdGZnh+RdDmwL3Bj/0uZmdlgKJIApqXn/XLjtvg0UEnbAltFxNr0+i3AqVuyTjMzK67I/QAOqtG2xwOXS6rE8eOI+HWNtmVmZj0UuRBsPPBfwISIOFTSZGD/iDh3SzYcEX8FXrkl6zAzs4ErUgzufGARMCEN/wk4sVYBmZlZffSZACRV9g7GRsQlpFNAI2I92ZW7ZmbWxPrbA7gtPT8laWdSHaB0b4Anah2YmZnVVn/HAJSeP012Cuhekm4BxgFH1TowMzOrrf4SwDhJn06vLweuIksKTwOHAL4ZzBDlWx2aWRH9JYARwBg27glUbFO7cGxLVW51WClzXLnVIeAkYGab6C8BrIoIX5jVZPq71aETgJnl9XcQuOcvf2sCvtWhmRXVXwI4uG5R2KDxrQ7NrKg+E0BEPFbPQGxwzJo+iZZRIzYZ51sdmllvihSDsybiWx2aWVFOAMOQb3VoZkUUqQVkZmbDkBOAmVlJOQGYmZWUE4CZWUk5AZiZlZQTgJlZSTkBmJmVlK8DKDGXjTYrNyeAknLZaDNzF1BJ9Vc22szKoaEJQNJbJS2X9BdJpzQylrJx2Wgza1gCkDQC+A5wKDAZOFbS5EbFUzYuG21mjdwD2Bf4S0T8NSKeAX4KHNHAeErFZaPNrJEJoA14IDf8YBq3CUkzJXVI6ujq6qpbcMPdjGltzD1yCm2tLQhoa21h7pFTfADYrESG/FlAETEfmA/Q3t4eDQ5nWHHZaLNya+QeQCewW2541zTOzMzqoJEJ4L+BvSXtKWk08C7gigbGY2ZWKg3rAoqI9ZI+DiwCRgDnRcRdjYrHzKxsGnoMICKuAq5qZAxmZmXlK4HNzErKCcDMrKScAMzMSsoJwMyspJwAzMxKygnAzKyknADMzErKCcDMrKScAMzMSsoJwMyspJwAzMxKygnAzKyknADMzErKCcDMrKScAMzMSsoJwMyspIb8TeG31MIlncxbtJyVq7uZ0NrCrOmTfCN0MzOGeQJYuKST2QuW0r1uAwCdq7uZvWApgJOAmZXesO4Cmrdo+XNf/hXd6zYwb9HyBkVkZjZ0DOsEsHJ1d1XjzczKZFgngAmtLVWNNzMrk2GdAGZNn0TLqBGbjGsZNYJZ0yc1KCIzs6GjIQlA0hxJnZLuSI+31WI7M6a1MffIKbS1tiCgrbWFuUdO8QFgMzMaexbQGRFxWq03MmNam7/wzcx6May7gMzMrG+NTAAfl3SnpPMk7djXTJJmSuqQ1NHV1VXP+MzMhjVFRG1WLF0DvKCXSf8B/AF4FAjgP4FdIuL9m1tne3t7dHR0DGqcZmbDnaTFEdHec3zNjgFExCFF5pN0DnBlreIwM7PeNeosoF1yg+8EljUiDjOzMqtZF1C/G5UuBKaSdQGtAD4cEasKLNcF3NfLpLFkXUrDwXBpy3BpB7gtQ9FwaQfUpy17RMS4niMbkgAGm6SO3vq3mtFwactwaQe4LUPRcGkHNLYtPg3UzKyknADMzEpquCSA+Y0OYBANl7YMl3aA2zIUDZd2QAPbMiyOAZiZWfWGyx6AmZlVyQnAzKykmioBSNpN0nWS7pZ0l6RPpvE7Sbpa0p/Tc5+1hYYKSVtLuk3S/6S2fCmN31PSrZL+IuliSaMbHWtRkkZIWiLpyjTcdG2RtELS0lSmvCONa7rPF4CkVkmXSfqjpHsk7d+MbZE0KVc6/g5JaySd2KRt+VT6f18m6Sfpe6Bh/ydNlQCA9cBJETEZ2A/4mKTJwCnAtRGxN3BtGh7qngbeFBGvJLso7q2S9gO+RlYq+0XA48AHGhhjtT4J3JMbbta2HBQRU3PnZjfj5wvgTODXEfES4JVk703TtSUilqf3YyrwauAfwOU0WVsktQGfANoj4uXACOBdNPL/JCKa9gH8HHgzsJysoBzALsDyRsdWZTu2AW4HXkN2ReDINH5/YFGj4yvYhl3J/gnfRFbbSc3YFrIr08f2GNd0ny9gB+BvpBM9mrktPeJ/C3BLM7YFaAMeAHYiq8N2JTC9kf8nzbYH8BxJE4FpwK3A+NhYSuIhYHyDwqpK6jK5A3gEuBq4F1gdEevTLA+SfWiawTeBzwLPpuGdac62BPAbSYslzUzjmvHztSfQBfwgdct9X9K2NGdb8t4F/CS9bqq2REQncBpwP7AKeAJYTAP/T5oyAUgaA/wMODEi1uSnRZZGm+Lc1ojYENlu7a7AvsBLGhzSgEg6DHgkIhY3OpZBcEBEvAo4lKyL8Q35iU30+RoJvAr4bkRMA56iRxdJE7UFgNQ3fjhwac9pzdCWdIziCLLkPAHYFnhrI2NqugQgaRTZl/9FEbEgjX64UmE0PT/SqPgGIiJWA9eR7f61SqqU6d4V6GxYYMW9Djhc0grgp2TdQGfShG1Jv9KIiEfI+pn3pTk/Xw8CD0bErWn4MrKE0IxtqTgUuD0iHk7DzdaWQ4C/RURXRKwDFpD97zTs/6SpEoAkAecC90TE6blJVwDHpdfHkR0bGNIkjZPUml63kB3LuIcsERyVZmuKtkTE7IjYNSImku2i/zYi/pUma4ukbSVtV3lN1t+8jCb8fEXEQ8ADkialUQcDd9OEbck5lo3dP9B8bbkf2E/SNum7rPKeNOz/pKmuBJZ0AHATsJSNfc2fIzsOcAmwO1m56KMj4rGGBFmQpFcAF5CdCbAVcElEnCrphWS/oncClgDviYinGxdpdSQdCHwmIg5rtrakeC9PgyOBH0fEVyTtTJN9vgAkTQW+D4wG/gq8j/RZo/nasi3ZF+gLI+KJNK7p3pd0uvcxZGc0LgE+SNbn35D/k6ZKAGZmNniaqgvIzMwGjxOAmVlJOQGYmZWUE4CZWUk5AZiZlZQTgPVK0s656osPSerMDQ+pqp6SDpT02hqu/3e1WnduG0/Wehu1JOlwSaek1zNSkUYb4nwaqG2WpDnAkxFxWgNjGJmrl9Jz2hyqjK+/9TWCpCcjYkyj4xgMks4HroyIyxodi/XPewBWmKRXS7ohFUpblLsM/3pJZ0jqSHXn95G0INVp/3KaZ2KqS39RmucySdsUWO83ldXl/6Skd6S66UskXSNpfCoK+BHgU2nv5PWSzpd0VC7uJ9PzgZJuknQFcHcqxjdP0n9LulPSh/tod37567Wxxv5F6YrOnvNfL6k9vR6bSmQg6fj0d/l1+tt8vZdlx0r6vaS397c9SQenv8NSSedJel7l756mHyGpW9JoZTXn/5qL7WvK7kXxJ0mv7yWGA5Xu6ZCGz5J0fHq9QtKXJN2etv2SXNvOSntihwPz0vuxl6RPKLuHx52Sftr7p8sawQnAihLwbeCoiHg1cB7wldz0ZyKrn3822aXsHwNeDhyv7IpNgEnA/4+IlwJrgBOU1Xbqb72jI6I9Ir4B3Azsl4qb/RT4bESsSNs8I7Ka8Tdtph2vAj4ZES8mq7v+RETsA+wDfEjSnptZfhpwIjAZeCFZLZdqTCW7EnQKcIyk3SoTJI0Hfgl8ISJ+2df2JG0NnA8cExFTyK5a/ijZVaRT03KvJytjsQ9ZmfFKTSDISg/vm9b7xSrjB3g0Fcz7LvCZ/ISI+B1ZiYZZ6f24l6wI3bSIeAVZsrYhYuTmZzED4HlkX+hXpx+hI8hK2lZckZ6XAndVyvSmX567AauBByLiljTfj8hujvHrzaz34tzrXYGL0x7CaLJ699W6LSIqy70FeEVub2EHYO/NrPe2iHgwte0OYCJZYirq2lwpg7uBPchqxI8iu5/CxyLihs1sby1ZUbE/pXkuSMt9U9K9kl5KVsTudOANZH/TfGKsFFFcnNZXrfzyRxaY/07gIkkLgYUD2J7ViBOAFSWyL/b9+5heqV3ybO51ZbjyOet5wCkKrPep3OtvA6dHxBXKag7N6WOZ9aS9W0lbkSWL3tYn4N8jYlEf6+lNvm0b6P1/6LntA1sXXH492RfqdOCGAvP35UayqpnrgGvI9hRGALN6WWeR+PtrQ5F4AN5OlojeAfyHpClD6fhLmbkLyIp6GhgnaX/IynJLelmV69i9sjzwbrJfzsurWO8ObCyVe1xu/Fpgu9zwCrJbB0LWHz2qj/UtAj6auqGQ9GJlRce2VH77R/UzX14A7wdeIunkzcy7HJgo6UVp+L1sTBo3kXXt/D4iushuzDOJrDuoqPuAyem4QitZ1cpqPPd+pAS8W0RcBxqn4LcAAADjSURBVJxM9h4Oi4Pdw4ETgBX1LNmX2dck/Q9wB1DtqZfLyW6ycg+wI9nNSp6pYr1zgEslLSa7jV7FL4B3Vg4CA+cAb0zr259Nf/XnfZ+sHO/tkpYB32Nw9opPI0ssS4CxRReKiA1kJY/fJOmEfub7J1llz0slVSrjnp0m30p2Z6wb0/CdwNKo4nS/iHiArMrmsvS8pOiyyU+BWan9ewM/SnEuAb6V7n9hQ4BPA7W6SGfrXBnZzbDNbAjwHoCZWUl5D8DMrKS8B2BmVlJOAGZmJeUEYGZWUk4AZmYl5QRgZlZS/wsYY7Bf0zSzLwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tt0vC_0Ey-fQ"
      },
      "source": [
        "## linear model \n",
        "def model (t_u, w , b):\n",
        "  # predicted temperature is \n",
        "  # weight times unknown temp\n",
        "  # plus bias \n",
        "  # w and b are scalars\n",
        "  # their values get broadcasted\n",
        "  # across the vector v_u\n",
        "  t_p = w*t_u +b \n",
        "  return t_p"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CM6rKglZ3hMJ"
      },
      "source": [
        "Now we need a loss function. A loss function is a function of the model parameters, that needs to decrease as learning progresses. This is how we know that we are learning (updating the model parameters) in the right direction. \n",
        "\n",
        "If the loss function is convex, then it will have a global minimum, and the argmin of the loss function will be the final value of the model parameters. \n",
        "\n",
        "Here we use the mean squared error between t_c (the observed temperature in celsius) and t_p(the temperature predicted by our model) as the loss function. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BEcUmYmy4PiY"
      },
      "source": [
        "def loss_function (t_c, t_p):\n",
        "\n",
        "  squared_error = (t_c - t_p)**2\n",
        "  mean_squared_error = squared_error.mean()\n",
        "  return mean_squared_error\n",
        "\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CJ2v9Foo3f9B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1405738-dd36-430d-fc60-87c34c765e35"
      },
      "source": [
        "# initialize weights and bias\n",
        "w = torch.ones([])\n",
        "b = torch.zeros([])\n",
        "\n",
        "# model forward pass\n",
        "t_p = w*t_u +b\n",
        "\n",
        "# calculate loss\n",
        "loss = loss_function(t_c, t_p)\n",
        "\n",
        "print(loss)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(1763.8848)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EKjEA-egM4GE"
      },
      "source": [
        "Gradient Descent: The basic idea is that we can find the minimum of the loss function by taking baby steps in the direction in which the function is decreasing. Of course in the case of the above convex function, it is easy to find the global minimum by differentiating the function and equating it to zero. In the interest of learning (our learning, not the model's) though, we'll try to get there by gradient descent. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zE-HVpDft1gs"
      },
      "source": [
        "# derivative of loss function with respect to t_p (model)\n",
        "def dloss_fn(t_c, t_p):\n",
        "  dloss_dtp = 2 * (t_p - t_c)/t_p.size(0)\n",
        "  return dloss_dtp\n",
        "\n",
        "# derivative of t_p(model) with respect to w\n",
        "def dmodel_dw(t_u, w, b):\n",
        "  return t_u \n",
        "\n",
        "# derivative of t_p(model) with respect to b\n",
        "def dmodel_db(t_u, w, b): \n",
        "   return 1.0 \n",
        "\n",
        "# final gradient depends on input, actuals, prediction and model parameters\n",
        "# final gradient is calculated by using the chain rule and the above gradients\n",
        "def gradient(t_u, t_c, t_p, w, b):\n",
        "    dloss_dw = dloss_fn(t_c, t_p)* dmodel_dw(t_u, w , b)\n",
        "    dloss_db = dloss_fn(t_c, t_p)*dmodel_db(t_u, w, b)\n",
        "    return dloss_dw.sum(), dloss_db.sum()\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fv1FnOALUtci"
      },
      "source": [
        "The training loop will now consist of iterating through \n",
        "1) A forward pass\n",
        "2) evaluation of gradients at the weights used in the forward pass\n",
        "3) updating the weights using the learning rate and gradients (backward pass)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HrSLiDe4VBWg"
      },
      "source": [
        "# training loop also depends on number of epochs and the learning rate\n",
        "def training_loop (n_epochs, learning_rate, t_u, t_c, t_p, params): \n",
        "  w,b = params\n",
        "  for epoch in range(1, n_epochs): \n",
        "    grad_w , grad_b = gradient(t_u, t_c, t_p , w, b) \n",
        "    w = w - learning_rate*grad_w\n",
        "    b = b - learning_rate*grad_b\n",
        "    t_p = model(t_u, w,b)\n",
        "    loss = loss_function(t_c, t_p)\n",
        "    if (epoch % 50 == 0): \n",
        "     print(\"Epoch number {0:.0f} , Loss is {1:.2f}\".format(epoch,loss))\n",
        "     print(\"Bias is {bias:0.4f} and Weight is {weight: 0.4f} \\n\\n\".format( bias = b,weight = w))\n",
        "\n",
        "  return w, b\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a6xoKcu6W7m5",
        "outputId": "ea0adb5a-d2bb-4ac7-aecb-e02906c388ea"
      },
      "source": [
        "# initialize weights and bias\n",
        "w = torch.ones([])\n",
        "b = torch.zeros([])\n",
        "training_loop(100, 0.0001, t_u, t_c, t_p , (w,b))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch number 50 , Loss is 29.07\n",
            "Bias is -0.0287 and Weight is  0.2325 \n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(0.2327), tensor(-0.0435))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5xR61FIgoJ0T"
      },
      "source": [
        ""
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bpV8S3O2oYV5"
      },
      "source": [
        "### Normalization of inputs: \n",
        "In the first few epochs we can notice that there is an order of magnitude difference between the weight and the bias term.\n",
        "To have a learning rate that works for both the weight and the bias, it is important to normalize these inputs. \n",
        "\n",
        "In the textbook, the authors use a simple rescaling factor of 0.1 for the input t_u. Let's try that.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BC4bndg3puyC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e946711a-b426-4118-aed9-cd2c06c820c1"
      },
      "source": [
        "w = torch.ones([])\n",
        "b = torch.zeros([])\n",
        "\n",
        "w,b = training_loop(5000,  1e-2, t_u*0.1, t_c, t_p, params = (w,b))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch number 50 , Loss is 25.64\n",
            "Bias is -1.2016 and Weight is  2.5231 \n",
            "\n",
            "\n",
            "Epoch number 100 , Loss is 22.09\n",
            "Bias is -2.5140 and Weight is  2.7549 \n",
            "\n",
            "\n",
            "Epoch number 150 , Loss is 19.09\n",
            "Bias is -3.7194 and Weight is  2.9678 \n",
            "\n",
            "\n",
            "Epoch number 200 , Loss is 16.57\n",
            "Bias is -4.8266 and Weight is  3.1634 \n",
            "\n",
            "\n",
            "Epoch number 250 , Loss is 14.43\n",
            "Bias is -5.8435 and Weight is  3.3431 \n",
            "\n",
            "\n",
            "Epoch number 300 , Loss is 12.63\n",
            "Bias is -6.7776 and Weight is  3.5081 \n",
            "\n",
            "\n",
            "Epoch number 350 , Loss is 11.12\n",
            "Bias is -7.6355 and Weight is  3.6596 \n",
            "\n",
            "\n",
            "Epoch number 400 , Loss is 9.84\n",
            "Bias is -8.4235 and Weight is  3.7988 \n",
            "\n",
            "\n",
            "Epoch number 450 , Loss is 8.76\n",
            "Bias is -9.1473 and Weight is  3.9267 \n",
            "\n",
            "\n",
            "Epoch number 500 , Loss is 7.84\n",
            "Bias is -9.8121 and Weight is  4.0441 \n",
            "\n",
            "\n",
            "Epoch number 550 , Loss is 7.08\n",
            "Bias is -10.4228 and Weight is  4.1520 \n",
            "\n",
            "\n",
            "Epoch number 600 , Loss is 6.43\n",
            "Bias is -10.9836 and Weight is  4.2511 \n",
            "\n",
            "\n",
            "Epoch number 650 , Loss is 5.88\n",
            "Bias is -11.4988 and Weight is  4.3421 \n",
            "\n",
            "\n",
            "Epoch number 700 , Loss is 5.42\n",
            "Bias is -11.9720 and Weight is  4.4257 \n",
            "\n",
            "\n",
            "Epoch number 750 , Loss is 5.03\n",
            "Bias is -12.4066 and Weight is  4.5024 \n",
            "\n",
            "\n",
            "Epoch number 800 , Loss is 4.70\n",
            "Bias is -12.8058 and Weight is  4.5730 \n",
            "\n",
            "\n",
            "Epoch number 850 , Loss is 4.42\n",
            "Bias is -13.1724 and Weight is  4.6377 \n",
            "\n",
            "\n",
            "Epoch number 900 , Loss is 4.19\n",
            "Bias is -13.5092 and Weight is  4.6972 \n",
            "\n",
            "\n",
            "Epoch number 950 , Loss is 3.99\n",
            "Bias is -13.8185 and Weight is  4.7519 \n",
            "\n",
            "\n",
            "Epoch number 1000 , Loss is 3.83\n",
            "Bias is -14.1026 and Weight is  4.8021 \n",
            "\n",
            "\n",
            "Epoch number 1050 , Loss is 3.69\n",
            "Bias is -14.3636 and Weight is  4.8482 \n",
            "\n",
            "\n",
            "Epoch number 1100 , Loss is 3.57\n",
            "Bias is -14.6033 and Weight is  4.8905 \n",
            "\n",
            "\n",
            "Epoch number 1150 , Loss is 3.47\n",
            "Bias is -14.8235 and Weight is  4.9294 \n",
            "\n",
            "\n",
            "Epoch number 1200 , Loss is 3.38\n",
            "Bias is -15.0257 and Weight is  4.9651 \n",
            "\n",
            "\n",
            "Epoch number 1250 , Loss is 3.31\n",
            "Bias is -15.2114 and Weight is  4.9979 \n",
            "\n",
            "\n",
            "Epoch number 1300 , Loss is 3.25\n",
            "Bias is -15.3820 and Weight is  5.0281 \n",
            "\n",
            "\n",
            "Epoch number 1350 , Loss is 3.20\n",
            "Bias is -15.5387 and Weight is  5.0557 \n",
            "\n",
            "\n",
            "Epoch number 1400 , Loss is 3.16\n",
            "Bias is -15.6827 and Weight is  5.0812 \n",
            "\n",
            "\n",
            "Epoch number 1450 , Loss is 3.12\n",
            "Bias is -15.8149 and Weight is  5.1045 \n",
            "\n",
            "\n",
            "Epoch number 1500 , Loss is 3.09\n",
            "Bias is -15.9363 and Weight is  5.1260 \n",
            "\n",
            "\n",
            "Epoch number 1550 , Loss is 3.07\n",
            "Bias is -16.0478 and Weight is  5.1457 \n",
            "\n",
            "\n",
            "Epoch number 1600 , Loss is 3.04\n",
            "Bias is -16.1503 and Weight is  5.1638 \n",
            "\n",
            "\n",
            "Epoch number 1650 , Loss is 3.03\n",
            "Bias is -16.2443 and Weight is  5.1804 \n",
            "\n",
            "\n",
            "Epoch number 1700 , Loss is 3.01\n",
            "Bias is -16.3308 and Weight is  5.1957 \n",
            "\n",
            "\n",
            "Epoch number 1750 , Loss is 3.00\n",
            "Bias is -16.4101 and Weight is  5.2097 \n",
            "\n",
            "\n",
            "Epoch number 1800 , Loss is 2.99\n",
            "Bias is -16.4831 and Weight is  5.2226 \n",
            "\n",
            "\n",
            "Epoch number 1850 , Loss is 2.98\n",
            "Bias is -16.5500 and Weight is  5.2344 \n",
            "\n",
            "\n",
            "Epoch number 1900 , Loss is 2.97\n",
            "Bias is -16.6115 and Weight is  5.2453 \n",
            "\n",
            "\n",
            "Epoch number 1950 , Loss is 2.96\n",
            "Bias is -16.6680 and Weight is  5.2552 \n",
            "\n",
            "\n",
            "Epoch number 2000 , Loss is 2.96\n",
            "Bias is -16.7199 and Weight is  5.2644 \n",
            "\n",
            "\n",
            "Epoch number 2050 , Loss is 2.95\n",
            "Bias is -16.7676 and Weight is  5.2728 \n",
            "\n",
            "\n",
            "Epoch number 2100 , Loss is 2.95\n",
            "Bias is -16.8114 and Weight is  5.2806 \n",
            "\n",
            "\n",
            "Epoch number 2150 , Loss is 2.95\n",
            "Bias is -16.8516 and Weight is  5.2877 \n",
            "\n",
            "\n",
            "Epoch number 2200 , Loss is 2.94\n",
            "Bias is -16.8885 and Weight is  5.2942 \n",
            "\n",
            "\n",
            "Epoch number 2250 , Loss is 2.94\n",
            "Bias is -16.9224 and Weight is  5.3002 \n",
            "\n",
            "\n",
            "Epoch number 2300 , Loss is 2.94\n",
            "Bias is -16.9536 and Weight is  5.3057 \n",
            "\n",
            "\n",
            "Epoch number 2350 , Loss is 2.94\n",
            "Bias is -16.9822 and Weight is  5.3107 \n",
            "\n",
            "\n",
            "Epoch number 2400 , Loss is 2.94\n",
            "Bias is -17.0085 and Weight is  5.3154 \n",
            "\n",
            "\n",
            "Epoch number 2450 , Loss is 2.93\n",
            "Bias is -17.0327 and Weight is  5.3196 \n",
            "\n",
            "\n",
            "Epoch number 2500 , Loss is 2.93\n",
            "Bias is -17.0548 and Weight is  5.3236 \n",
            "\n",
            "\n",
            "Epoch number 2550 , Loss is 2.93\n",
            "Bias is -17.0752 and Weight is  5.3272 \n",
            "\n",
            "\n",
            "Epoch number 2600 , Loss is 2.93\n",
            "Bias is -17.0939 and Weight is  5.3305 \n",
            "\n",
            "\n",
            "Epoch number 2650 , Loss is 2.93\n",
            "Bias is -17.1111 and Weight is  5.3335 \n",
            "\n",
            "\n",
            "Epoch number 2700 , Loss is 2.93\n",
            "Bias is -17.1269 and Weight is  5.3363 \n",
            "\n",
            "\n",
            "Epoch number 2750 , Loss is 2.93\n",
            "Bias is -17.1414 and Weight is  5.3389 \n",
            "\n",
            "\n",
            "Epoch number 2800 , Loss is 2.93\n",
            "Bias is -17.1547 and Weight is  5.3412 \n",
            "\n",
            "\n",
            "Epoch number 2850 , Loss is 2.93\n",
            "Bias is -17.1669 and Weight is  5.3434 \n",
            "\n",
            "\n",
            "Epoch number 2900 , Loss is 2.93\n",
            "Bias is -17.1782 and Weight is  5.3454 \n",
            "\n",
            "\n",
            "Epoch number 2950 , Loss is 2.93\n",
            "Bias is -17.1885 and Weight is  5.3472 \n",
            "\n",
            "\n",
            "Epoch number 3000 , Loss is 2.93\n",
            "Bias is -17.1980 and Weight is  5.3488 \n",
            "\n",
            "\n",
            "Epoch number 3050 , Loss is 2.93\n",
            "Bias is -17.2067 and Weight is  5.3504 \n",
            "\n",
            "\n",
            "Epoch number 3100 , Loss is 2.93\n",
            "Bias is -17.2147 and Weight is  5.3518 \n",
            "\n",
            "\n",
            "Epoch number 3150 , Loss is 2.93\n",
            "Bias is -17.2220 and Weight is  5.3531 \n",
            "\n",
            "\n",
            "Epoch number 3200 , Loss is 2.93\n",
            "Bias is -17.2287 and Weight is  5.3543 \n",
            "\n",
            "\n",
            "Epoch number 3250 , Loss is 2.93\n",
            "Bias is -17.2349 and Weight is  5.3554 \n",
            "\n",
            "\n",
            "Epoch number 3300 , Loss is 2.93\n",
            "Bias is -17.2406 and Weight is  5.3564 \n",
            "\n",
            "\n",
            "Epoch number 3350 , Loss is 2.93\n",
            "Bias is -17.2459 and Weight is  5.3573 \n",
            "\n",
            "\n",
            "Epoch number 3400 , Loss is 2.93\n",
            "Bias is -17.2507 and Weight is  5.3582 \n",
            "\n",
            "\n",
            "Epoch number 3450 , Loss is 2.93\n",
            "Bias is -17.2551 and Weight is  5.3589 \n",
            "\n",
            "\n",
            "Epoch number 3500 , Loss is 2.93\n",
            "Bias is -17.2591 and Weight is  5.3597 \n",
            "\n",
            "\n",
            "Epoch number 3550 , Loss is 2.93\n",
            "Bias is -17.2628 and Weight is  5.3603 \n",
            "\n",
            "\n",
            "Epoch number 3600 , Loss is 2.93\n",
            "Bias is -17.2663 and Weight is  5.3609 \n",
            "\n",
            "\n",
            "Epoch number 3650 , Loss is 2.93\n",
            "Bias is -17.2694 and Weight is  5.3615 \n",
            "\n",
            "\n",
            "Epoch number 3700 , Loss is 2.93\n",
            "Bias is -17.2723 and Weight is  5.3620 \n",
            "\n",
            "\n",
            "Epoch number 3750 , Loss is 2.93\n",
            "Bias is -17.2749 and Weight is  5.3624 \n",
            "\n",
            "\n",
            "Epoch number 3800 , Loss is 2.93\n",
            "Bias is -17.2774 and Weight is  5.3629 \n",
            "\n",
            "\n",
            "Epoch number 3850 , Loss is 2.93\n",
            "Bias is -17.2796 and Weight is  5.3633 \n",
            "\n",
            "\n",
            "Epoch number 3900 , Loss is 2.93\n",
            "Bias is -17.2816 and Weight is  5.3636 \n",
            "\n",
            "\n",
            "Epoch number 3950 , Loss is 2.93\n",
            "Bias is -17.2835 and Weight is  5.3640 \n",
            "\n",
            "\n",
            "Epoch number 4000 , Loss is 2.93\n",
            "Bias is -17.2853 and Weight is  5.3643 \n",
            "\n",
            "\n",
            "Epoch number 4050 , Loss is 2.93\n",
            "Bias is -17.2868 and Weight is  5.3645 \n",
            "\n",
            "\n",
            "Epoch number 4100 , Loss is 2.93\n",
            "Bias is -17.2883 and Weight is  5.3648 \n",
            "\n",
            "\n",
            "Epoch number 4150 , Loss is 2.93\n",
            "Bias is -17.2897 and Weight is  5.3650 \n",
            "\n",
            "\n",
            "Epoch number 4200 , Loss is 2.93\n",
            "Bias is -17.2909 and Weight is  5.3653 \n",
            "\n",
            "\n",
            "Epoch number 4250 , Loss is 2.93\n",
            "Bias is -17.2920 and Weight is  5.3655 \n",
            "\n",
            "\n",
            "Epoch number 4300 , Loss is 2.93\n",
            "Bias is -17.2931 and Weight is  5.3656 \n",
            "\n",
            "\n",
            "Epoch number 4350 , Loss is 2.93\n",
            "Bias is -17.2940 and Weight is  5.3658 \n",
            "\n",
            "\n",
            "Epoch number 4400 , Loss is 2.93\n",
            "Bias is -17.2949 and Weight is  5.3660 \n",
            "\n",
            "\n",
            "Epoch number 4450 , Loss is 2.93\n",
            "Bias is -17.2957 and Weight is  5.3661 \n",
            "\n",
            "\n",
            "Epoch number 4500 , Loss is 2.93\n",
            "Bias is -17.2964 and Weight is  5.3662 \n",
            "\n",
            "\n",
            "Epoch number 4550 , Loss is 2.93\n",
            "Bias is -17.2971 and Weight is  5.3664 \n",
            "\n",
            "\n",
            "Epoch number 4600 , Loss is 2.93\n",
            "Bias is -17.2977 and Weight is  5.3665 \n",
            "\n",
            "\n",
            "Epoch number 4650 , Loss is 2.93\n",
            "Bias is -17.2983 and Weight is  5.3666 \n",
            "\n",
            "\n",
            "Epoch number 4700 , Loss is 2.93\n",
            "Bias is -17.2988 and Weight is  5.3667 \n",
            "\n",
            "\n",
            "Epoch number 4750 , Loss is 2.93\n",
            "Bias is -17.2993 and Weight is  5.3668 \n",
            "\n",
            "\n",
            "Epoch number 4800 , Loss is 2.93\n",
            "Bias is -17.2998 and Weight is  5.3668 \n",
            "\n",
            "\n",
            "Epoch number 4850 , Loss is 2.93\n",
            "Bias is -17.3002 and Weight is  5.3669 \n",
            "\n",
            "\n",
            "Epoch number 4900 , Loss is 2.93\n",
            "Bias is -17.3005 and Weight is  5.3670 \n",
            "\n",
            "\n",
            "Epoch number 4950 , Loss is 2.93\n",
            "Bias is -17.3009 and Weight is  5.3670 \n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1cPVzSLq4X4"
      },
      "source": [
        "With normalization it was possible to use a larger learning rate of 0.2 and not have the parameters explode. Setting n_epochs to 5000, we can see that the values closely resemble the parameters for converting Fahrenheit to Celsius. \n",
        "The actual values are (w = 5.5556, b = -17.7778). Looks like the fancy wall thermometer was showing the temperature in Fahrenheit"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ahLzxA5Fq5GR"
      },
      "source": [
        "Revisualizing the data using matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 379
        },
        "id": "kLEb-kaYtb2M",
        "outputId": "f7bdd14c-86bf-42c8-e942-fce14f01ea24"
      },
      "source": [
        "# we scale the input because the model in normalized\n",
        "t_p = model(t_u*0.1, w, b)\n",
        "\n",
        "plt.plot(t_u.numpy(), t_p.detach().numpy())\n",
        "plt.plot(t_u.numpy(), t_c.numpy(), 'o')\n",
        "plt.title(\"Convert fancy thermometer units to celsius\")\n",
        "plt.xlabel('Temperature in unknown unit(probably fahrenheit')\n",
        "plt.ylabel('Temperature in celsius')\n",
        "plt.show()\n",
        "print(t_p)\n",
        "print(w, b)\n",
        "print(t_u*w + b)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([ 1.8593, 12.7008, 13.9352, 26.6552, 12.9155,  8.9439,  0.8932, -5.6009,\n",
            "         8.6755, 15.1160, 19.4097])\n",
            "tensor(5.3671) tensor(-17.3012)\n",
            "tensor([174.3037, 282.7188, 295.0630, 422.2629, 284.8656, 245.1492, 164.6429,\n",
            "         99.7012, 242.4656, 306.8706, 349.8073])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MMLMU1LDYNJi"
      },
      "source": [
        ""
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YN1eqfwdaB9E"
      },
      "source": [
        "Pytorch and automatic differentiation.\n",
        "Gradients in pytorch can be calculated via automatic differentiation, instead of manually defining formulae for them. \n",
        "The textbook reworks the above example using automatic differentiation. \n",
        "I will first take a break from this problem to explore automatic differentiation with tensors. We will then get back to the above problem and solve it using automatic differentiation. \n",
        "\n",
        "We can keep the product rule and chain rule of differntiation in mind for checking pytorch's answer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xnUTdU-1y85A",
        "outputId": "fdbd4732-2334-486d-ee01-a99dcc4192b2"
      },
      "source": [
        "## create a tensor\n",
        "# The parameter requires_grad needs to be set to True, for gradients to accumulate. \n",
        "original = torch.tensor([1,2,3], dtype = torch.float64, requires_grad = True)\n",
        "operand1 = torch.tensor([3,2,1], dtype = torch.float64, requires_grad = True)\n",
        "operand2 = torch.tensor([4,5,6], dtype = torch.float64, requires_grad = True )\n",
        "\n",
        "## the backward pass only works for scalars, so we have created a scalar dor product here\n",
        "dot_product1 = original.dot(operand1)\n",
        "dot_product2 = original.dot(operand2)\n",
        "\n",
        "print(dot_product1, dot_product2)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(10., dtype=torch.float64, grad_fn=<DotBackward>) tensor(32., dtype=torch.float64, grad_fn=<DotBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yX1eWjEC3Ad3",
        "outputId": "9abc43b9-f82b-4d95-f3ad-af6c47fcd29e"
      },
      "source": [
        "#retain_graph needs to be set to True if we intent to call the backward pass again. \n",
        "#Which we may decide to do to learn  \n",
        "dot_product1.backward(retain_graph = True)\n",
        "original.grad"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([3., 2., 1.], dtype=torch.float64)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hC4iIrRx3eUA",
        "outputId": "b9b68d29-cd5a-4618-d62a-d1f2f51e5aaf"
      },
      "source": [
        "# if the gradient of original is not reset to zero,gradients from future calculations\n",
        "# like the one below will accumulate on top of previous calculations. Comment out \n",
        "# the .zero_() function to observe this \n",
        "original.grad.zero_()\n",
        "dot_product2.backward(retain_graph = True)\n",
        "original.grad"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([4., 5., 6.], dtype=torch.float64)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wxc_JFQy3pZ3",
        "outputId": "7f47a044-3d8b-4305-83b4-1265a43ccc36"
      },
      "source": [
        "original.grad.zero_()\n",
        "\n",
        "# change the below from + to * , calculate gradients by hand\n",
        "# compare with pytorch's answer \n",
        "\n",
        "output = dot_product1 *dot_product2\n",
        "output.backward(retain_graph = True)\n",
        "original.grad\n",
        "\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([136., 114.,  92.], dtype=torch.float64)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0hMXDstO7Lz8"
      },
      "source": [
        "Now that I've understood how autograd works. I'm going to try to build the learning model for temperature conversion using autograd"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W_zYE9uK_IiB"
      },
      "source": [
        "## training function rewritten using autograd\n",
        "\n",
        "def training_loop_auto(n_epochs, learning_rate, t_u, t_c, params): \n",
        "\n",
        "  for epoch in range(0, n_epochs): \n",
        "    # zero out the parameter gradients. \n",
        "    if params.grad is not None: \n",
        "        params.grad.zero_()    \n",
        "    #w, b = params[0] , params[1]\n",
        "    t_p = model(t_u, *params )\n",
        "    \n",
        "    loss = loss_function(t_c, t_p)\n",
        "    loss.backward(retain_graph = True)\n",
        "    # here we are telling pytorch to not include the\n",
        "    # below computation in the forward graph\n",
        "\n",
        "    with torch.no_grad():\n",
        "      if params.grad is not None: \n",
        "          # params updated in place      \n",
        "          params-= learning_rate*params.grad\n",
        "          \n",
        "\n",
        "    if (epoch % 500 == 0):\n",
        "      print(\"Epoch number {epoch_num:d} has loss {loss_value:0.2f}\".format(epoch_num = epoch, loss_value = loss))\n",
        "      print(\"Bias is  {bias:0.2f} has weight is {weight:0.2f}\\n\\n\".format(bias = params[0], weight = params[1]))\n",
        "\n",
        "  return params \n",
        "\n",
        "## Important: I learned that params = params- learning_rate*params.grad \n",
        "## does not update params in place, a different variable called params is created\n",
        "## and assigned the value params - learning_rate*params.grad. \n",
        "## This variable did not have requires_grad = True\n",
        "## My code failed and I did not know why for the longest time\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pDo89ThwSgJT",
        "outputId": "00914100-12bb-4090-b397-4a89fcdfb10b"
      },
      "source": [
        "training_loop_auto(\n",
        "    n_epochs = 5000, \n",
        "    learning_rate = 1e-2, \n",
        "    params = torch.tensor([1.0, 0.0], requires_grad=True), # <1> \n",
        "    t_u = t_u*0.1, # <2> \n",
        "    t_c = t_c)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch number 0 has loss 80.36\n",
            "Bias is  1.78 has weight is 0.11\n",
            "\n",
            "\n",
            "Epoch number 500 has loss 7.84\n",
            "Bias is  4.05 has weight is -9.83\n",
            "\n",
            "\n",
            "Epoch number 1000 has loss 3.83\n",
            "Bias is  4.80 has weight is -14.11\n",
            "\n",
            "\n",
            "Epoch number 1500 has loss 3.09\n",
            "Bias is  5.13 has weight is -15.94\n",
            "\n",
            "\n",
            "Epoch number 2000 has loss 2.96\n",
            "Bias is  5.26 has weight is -16.72\n",
            "\n",
            "\n",
            "Epoch number 2500 has loss 2.93\n",
            "Bias is  5.32 has weight is -17.06\n",
            "\n",
            "\n",
            "Epoch number 3000 has loss 2.93\n",
            "Bias is  5.35 has weight is -17.20\n",
            "\n",
            "\n",
            "Epoch number 3500 has loss 2.93\n",
            "Bias is  5.36 has weight is -17.26\n",
            "\n",
            "\n",
            "Epoch number 4000 has loss 2.93\n",
            "Bias is  5.36 has weight is -17.29\n",
            "\n",
            "\n",
            "Epoch number 4500 has loss 2.93\n",
            "Bias is  5.37 has weight is -17.30\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([  5.3671, -17.3012], requires_grad=True)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RvFEykknroJv"
      },
      "source": [
        "## Optimizers\n",
        "\n",
        "Pytorch provides a variety of optimizers, so we don't have to implement gradient descent ourselves. These optimizers also have their own strategies to assist with model convergence. \n",
        "Let us take a look at some of the available optimizers. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "anV0uVrk-LDX",
        "outputId": "9b5ab553-20bb-455e-dca4-8d9ae523ba89"
      },
      "source": [
        "import torch.optim as optim \n",
        "dir(optim)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['ASGD',\n",
              " 'Adadelta',\n",
              " 'Adagrad',\n",
              " 'Adam',\n",
              " 'AdamW',\n",
              " 'Adamax',\n",
              " 'LBFGS',\n",
              " 'Optimizer',\n",
              " 'RMSprop',\n",
              " 'Rprop',\n",
              " 'SGD',\n",
              " 'SparseAdam',\n",
              " '__builtins__',\n",
              " '__cached__',\n",
              " '__doc__',\n",
              " '__file__',\n",
              " '__loader__',\n",
              " '__name__',\n",
              " '__package__',\n",
              " '__path__',\n",
              " '__spec__',\n",
              " '_functional',\n",
              " '_multi_tensor',\n",
              " 'lr_scheduler',\n",
              " 'swa_utils']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YOX_hGJZ-Qfd"
      },
      "source": [
        "# the optimizer object's constructor takes the model's parameter tensor and the learning rate as its'parameters\n",
        "params = torch.tensor([1.0, 0.0], requires_grad = True)\n",
        "learning_rate = 0.01\n",
        "\n",
        "# sgd stands for stochastic gradient descent. \n",
        "# since our batch size in this case is the entire dataset, we aren't using\n",
        "# this optimizer stochastically \n",
        "\n",
        "sgd_optimizer = optim.SGD([params], lr = learning_rate)\n",
        "\n"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ubTyQM4SttaJ"
      },
      "source": [
        "#rewriting the training loop\n",
        "\n",
        "def training_loop_with_optim(n_epochs,t_c, t_u, params, learning_rate, opt):\n",
        "\n",
        "  for epoch in range(1,n_epochs): \n",
        "      t_p = model(t_u, *params) \n",
        "      loss = loss_function(t_c, t_p)\n",
        "      #print(loss, params.grad)\n",
        "      opt.zero_grad()\n",
        "      loss.backward()\n",
        "      opt.step()\n",
        "     \n",
        "\n",
        "      if epoch% 500 == 0:\n",
        "          print('Epoch {epoch_num: d} Loss is {loss_value:0.2f}'.format(epoch_num=epoch, loss_value = loss))\n",
        "          print('Parameters are {params}'.format(params = params))\n",
        "\n",
        "  return params\n",
        "  \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N7SzeVzkIr8A",
        "outputId": "ee505257-06d0-44de-a185-02173791f900"
      },
      "source": [
        "params = torch.tensor([1.0,0.0], requires_grad = True)\n",
        "opt = optim.SGD([params], lr = 0.01)\n",
        "training_loop_with_optim(5000, t_c, t_u*0.1, params, learning_rate, opt)\n"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch  500 Loss is 7.86\n",
            "Parameters are tensor([ 4.0443, -9.8133], requires_grad=True)\n",
            "Epoch  1000 Loss is 3.83\n",
            "Parameters are tensor([  4.8021, -14.1031], requires_grad=True)\n",
            "Epoch  1500 Loss is 3.09\n",
            "Parameters are tensor([  5.1260, -15.9365], requires_grad=True)\n",
            "Epoch  2000 Loss is 2.96\n",
            "Parameters are tensor([  5.2644, -16.7200], requires_grad=True)\n",
            "Epoch  2500 Loss is 2.93\n",
            "Parameters are tensor([  5.3236, -17.0549], requires_grad=True)\n",
            "Epoch  3000 Loss is 2.93\n",
            "Parameters are tensor([  5.3489, -17.1980], requires_grad=True)\n",
            "Epoch  3500 Loss is 2.93\n",
            "Parameters are tensor([  5.3597, -17.2591], requires_grad=True)\n",
            "Epoch  4000 Loss is 2.93\n",
            "Parameters are tensor([  5.3643, -17.2853], requires_grad=True)\n",
            "Epoch  4500 Loss is 2.93\n",
            "Parameters are tensor([  5.3662, -17.2964], requires_grad=True)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([  5.3671, -17.3012], requires_grad=True)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fpDiGBd_L3Y9"
      },
      "source": [
        "With other optimizers like Adam, which uses an adaptive learning rate, we will not have to scale our inputs. Trying out adam below"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KJsoOvsyMCIk",
        "outputId": "ac9bd748-5d16-4267-ef4a-05f09856da15"
      },
      "source": [
        "# Adam converges way faster and can take a much larger learning rate\n",
        "\n",
        "params = torch.tensor([1.0,0.0], requires_grad = True)\n",
        "opt = optim.Adam([params], lr = 0.1)\n",
        "training_loop_with_optim(1500, t_c, t_u, params, learning_rate, opt)\n"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch  500 Loss is 7.61\n",
            "Parameters are tensor([  0.4081, -10.0095], requires_grad=True)\n",
            "Epoch  1000 Loss is 3.09\n",
            "Parameters are tensor([  0.5131, -15.9629], requires_grad=True)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([  0.5350, -17.2015], requires_grad=True)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gz5UUE4oMdYY"
      },
      "source": [
        "### Validation Set\n",
        "\n",
        "To prevent overfitting the model by minimizing training loss alone, it makes sense to set aside some of the data as a validation set. \n",
        "Best lectures I've seen on overfitting are by Professor Yasser Abu Moustafa of Caltech. \n",
        "Refer the below link: \n",
        "* https://www.youtube.com/watch?v=EQWr3GGCdzw&list=PLD63A284B7615313A&index=11&ab_channel=caltech\n",
        "\n",
        "I suggest watching all his lectures, he explains generalization and learning better than almost anyone else. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fm4VLC6EQRJC"
      },
      "source": [
        "n_samples = t_u.shape[0]\n",
        "n_val = int(0.2* n_samples)\n",
        "\n",
        "shuffled_indices = torch.randperm(n_samples)\n",
        "\n",
        "train_indices = shuffled_indices[0:-n_val]\n",
        "val_indices = shuffled_indices[-n_val:n_samples]"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yhX7YCYzQd6c"
      },
      "source": [
        "# I am deviating from the textbook a little here\n",
        "# I am setting aside a validation sample within the training loop\n",
        "# because this is ultimately for illustrative purposes\n",
        "# and I got tired of passing too many arguments \n",
        "\n",
        "def training_loop_with_val(n_epochs, t_u, t_c, params, optim): \n",
        "\n",
        "  n_samples = t_u.shape[0]\n",
        "  n_val = int(n_samples * 0.2)\n",
        "  shuffled_indices = torch.randperm(n_samples)\n",
        "\n",
        "  train_indices = shuffled_indices[:-n_val]\n",
        "  val_indices = shuffled_indices[-n_val:n_samples]\n",
        "\n",
        "  train_t_u = t_u[train_indices]\n",
        "  val_t_u = t_u[val_indices]\n",
        "  \n",
        "  train_t_c = t_c[train_indices]\n",
        "  val_t_c = t_c[val_indices]\n",
        "\n",
        "  for epoch in range(1,n_epochs): \n",
        "    train_t_p = model(train_t_u, *params)\n",
        "    val_t_p = model(val_t_u, *params)\n",
        "    train_loss = loss_function(train_t_c, train_t_p)\n",
        "    train_loss.backward()\n",
        "    optim.step()\n",
        "    optim.zero_grad()\n",
        "\n",
        "    ## we use torch.no_grad() here to retaining an expensive forward graph\n",
        "    ## for val_loss which we won't use, ultimately. \n",
        "    \n",
        "    with torch.no_grad():\n",
        "        val_loss = loss_function(val_t_c, val_t_p)\n",
        "    \n",
        "    if(epoch% 500 == 0): \n",
        "      print(\"Epoch {epoch:d} Training loss is {loss_value: 0.2f}\".format(epoch = epoch, loss_value = train_loss))\n",
        "      print(\"Epoch {epoch:d} Validation loss is {loss_value: 0.2f}\".format(epoch = epoch, loss_value = val_loss))\n",
        "\n",
        "      print(\"Parameters are {params}\".format(params = params))\n",
        "  return params\n",
        "\n"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5iOd2iWpYU_D",
        "outputId": "8f8a4bdd-1c15-402d-c4c8-50e72f3cb453"
      },
      "source": [
        "from torch import optim \n",
        "params = torch.tensor([1.0, 0.0], requires_grad = True)\n",
        "optim_SGD = optim.SGD([params], lr = 0.01)\n",
        "training_loop_with_val(5000,t_u*0.1, t_c, params, optim_SGD)"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 500 Training loss is  8.31\n",
            "Epoch 500 Validation loss is  1.08\n",
            "Parameters are tensor([  4.1501, -10.5170], requires_grad=True)\n",
            "Epoch 1000 Training loss is  3.77\n",
            "Epoch 1000 Validation loss is  1.81\n",
            "Parameters are tensor([  4.9339, -15.0677], requires_grad=True)\n",
            "Epoch 1500 Training loss is  2.96\n",
            "Epoch 1500 Validation loss is  3.20\n",
            "Parameters are tensor([  5.2660, -16.9956], requires_grad=True)\n",
            "Epoch 2000 Training loss is  2.81\n",
            "Epoch 2000 Validation loss is  3.99\n",
            "Parameters are tensor([  5.4067, -17.8125], requires_grad=True)\n",
            "Epoch 2500 Training loss is  2.78\n",
            "Epoch 2500 Validation loss is  4.36\n",
            "Parameters are tensor([  5.4663, -18.1585], requires_grad=True)\n",
            "Epoch 3000 Training loss is  2.78\n",
            "Epoch 3000 Validation loss is  4.52\n",
            "Parameters are tensor([  5.4915, -18.3051], requires_grad=True)\n",
            "Epoch 3500 Training loss is  2.78\n",
            "Epoch 3500 Validation loss is  4.59\n",
            "Parameters are tensor([  5.5022, -18.3672], requires_grad=True)\n",
            "Epoch 4000 Training loss is  2.78\n",
            "Epoch 4000 Validation loss is  4.62\n",
            "Parameters are tensor([  5.5067, -18.3936], requires_grad=True)\n",
            "Epoch 4500 Training loss is  2.78\n",
            "Epoch 4500 Validation loss is  4.63\n",
            "Parameters are tensor([  5.5087, -18.4047], requires_grad=True)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([  5.5095, -18.4094], requires_grad=True)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q55oaSOgactz"
      },
      "source": [
        "#### That's it for Chapter 5!   \n",
        "#### See you in Chapter 6! \n"
      ]
    }
  ]
}